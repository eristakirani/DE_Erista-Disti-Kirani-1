# **Nama: Erista Disti Kirani** 


**Summary Pengenalan Data Engineer** 


- DE role definition and the demand of DE 
- Data Pipeline definition 
- The difference between ETL and ELT data stack 
- The different use case of OLTP and OLAP 

Data Engineer 

Data Engineer is the process of taking raw data and cleaningit, transforming it, analyzing it, and publishing it to make it meaningful and useful to people and organizations 

this job requires a handful of skills: 

- python 
- SQL 
- cloud (AWS, GCP) 
- the standard Big Data tools and platforms such as Apache Hadoop, scala and Apache Hive. 

Data is The New Oil 

- Data is the new oil because data can be used to derive insights. 

  Insights can drive customer retention, upselling, new revenue models, advertising, etc. 

- id data is the new oil, insights are the new money. 

Raw Data, where does it come from? 

- only the data analyst and data engineer who can wrangle and make sense of gobs of information coming from an ever-increaing number of sources. 

- Raw data example: 

  ` `- consumer software system (Tiktok, spotify, amazon) 

  ` `- Internal system (Salesforce, CRM, Accounting, HR, etc).

  ` `- Internal business users (excel, spreadsheet) 

  ` `- IoT devices (solar panels, automobiles, cell phone)


Data Pipeline Definition 

- A data pipeline is a series of processing steps to prepare enterprise data for analysis

Types of Data Pipeline 

- Order of transformation flows 

  `	`ETL (Extract-Transform-Load) 

  `	`ELT (Extract-Load-Transform) 

- Source of data 

  `	`Batch Pipeline 

  `	`Stream Pipeline

- ETL (Extract-Transform-Load) 

  Data pipeline extract raw data from multiple sources 

  stores it in a temporary location called a staging area. 

  transform data in the staging area 

  loads it into data lakes or warehouses. 

- ELT (Extract-Load-Transform) 

  Data pipeline extract and loads raw data directly into a data lake

  they perform transformations/changes after moving the information to data warehouse

Type of Data Pipeline 

- Batch data pipeline 

  ` `80% data engineering use cases requires batch processing. 

  ` `Batch processing is when the processing and analysis happens on a set of data that   have already been stored over a period of time. 

  ` `Example: payroll and billing system that have to be processed weekly or month. 

- Stream Data pipeline 

  Sreaming data pipeline is the sequential process of ingests, processes, andmanages continuous streams of data while theyâ€™re still in mention. 

  This results in analysis and reporting of events as it happens or near rea;-time 

  Example: Fraud detection or intrusion detection. 

What is Streaming Data? 

- Streaming data are unending, continuous resources of data, which typically send in the data records simultaneously. 
- Example: in-game player activity, information from social networks, financial trading floors, user clicking on a link in a web page, data from sensors. 






 
 

